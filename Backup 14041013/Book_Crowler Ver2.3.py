# -*- coding: utf-8 -*-
"""
Book_Crawler_v2.3
- Ø¬Ø¯Ø§Ø´Ø¯Ù† Ù…Ù†Ø·Ù‚: get_book_div_from_page Ùˆ extract_details_from_div
- Ø¯Ø§Ù†Ù„ÙˆØ¯ ØªØµÙˆÛŒØ± Ø¨Ø§ Ù†Ø§Ù… Ø´Ø§Ø¨Ú© (Ø¨Ø¯ÙˆÙ† -) Ø¨Ù‡ Ù¾ÙˆØ´Ù‡ "Books Images"
- Ø¯Ø±Ø¬ ØªØµÙˆÛŒØ± Ø¯Ø± Ø§Ú©Ø³Ù„ Ø¨Ù‡ ØµÙˆØ±Øª LinkToFile Ùˆ Placement = MoveAndSize
- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² html.parser (Ø¨Ø¯ÙˆÙ† lxml)
"""
import os
import re
import time
import random
import requests
import pandas as pd
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import sys

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª
EXCEL_FILE = "Parsa Library.xlsx"
IMAGE_DIR = "Books Images"
BASE = "https://www.iranketab.ir"
HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
DELAY_RANGE = (0.8, 2.0)
UPDATE_ALL = False
EXCEL_VISIBLE = True

os.makedirs(IMAGE_DIR, exist_ok=True)

def log(msg):
    print(f"[{time.strftime('%H:%M:%S')}] {msg}")

def remove_old_images(ws, row, col):
    """
    ØªÙ…Ø§Ù… ØªØµØ§ÙˆÛŒØ± ÛŒØ§ ShapeÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ø± Ø³Ù„ÙˆÙ„ Ù…Ø´Ø®Øµ Ø´Ø¯Ù‡ (row, col) Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ù†Ø¯ Ø±Ø§ Ø­Ø°Ù Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø¨Ù‡Ù… Ø±ÛŒØ®ØªÙ† Ø§ÛŒÙ†Ø¯Ú©Ø³â€ŒÙ‡Ø§ Ù‡Ù†Ú¯Ø§Ù… Ø­Ø°ÙØŒ Ø§Ø² Ø§Ù†ØªÙ‡Ø§ÛŒ Ù„ÛŒØ³Øª Ø´Ø±ÙˆØ¹ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    """
    try:
        # ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ ShapeÙ‡Ø§
        count = ws.Shapes.Count
        # Ø­Ù„Ù‚Ù‡ Ù…Ø¹Ú©ÙˆØ³ (Ø§Ø² Ø¢Ø®Ø± Ø¨Ù‡ Ø§ÙˆÙ„)
        for i in range(count, 0, -1):
            shp = ws.Shapes.Item(i)
            # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¢ÛŒØ§ "Ø³Ù„ÙˆÙ„Ù Ù„Ù†Ú¯Ø±" ØªØµÙˆÛŒØ±ØŒ Ù‡Ù…Ø§Ù† Ø³Ù„ÙˆÙ„ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ù…Ø§Ø³ØªØŸ
            if shp.TopLeftCell.Row == row and shp.TopLeftCell.Column == col:
                shp.Delete()
    except Exception as e:
        print(f"Error removing old images: {e}")
        
def safe_get(url, allow_redirects=True, timeout=25):
    """
    Ø¯Ø±Ø®ÙˆØ§Ø³Øª HTTP Ø§ÛŒÙ…Ù† Ø¨Ø§ fallback Ø®ÙˆØ¯Ú©Ø§Ø± Ø¨Ø±Ø§ÛŒ Ø®Ø·Ø§ÛŒ SSL.
    Ø§Ø¨ØªØ¯Ø§ Ø¨Ø§ verify=True ØªÙ„Ø§Ø´ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ø¯Ø± ØµÙˆØ±Øª SSLError Ù…Ø¬Ø¯Ø¯ Ø¨Ø§ verify=False Ø§Ù…ØªØ­Ø§Ù† Ù…ÛŒâ€ŒØ´ÙˆØ¯.
    """
    try:
        time.sleep(random.uniform(*DELAY_RANGE))
        # ØªÙ„Ø§Ø´ Ø§ÙˆÙ„ Ø¨Ø§ ØªØ£ÛŒÛŒØ¯ SSL
        r = requests.get(url, headers=HEADERS, allow_redirects=allow_redirects, timeout=timeout, verify=True)
        r.raise_for_status()
        r.encoding = "utf-8"
        return r
    except requests.exceptions.SSLError as e:
        log(f"âš ï¸ Ù‡Ø´Ø¯Ø§Ø± SSL Ø¯Ø± GET {url}: {e} â†’ ØªÙ„Ø§Ø´ Ù…Ø¬Ø¯Ø¯ Ø¨Ø¯ÙˆÙ† ØªØ£ÛŒÛŒØ¯ SSL ...")
        try:
            r = requests.get(url, headers=HEADERS, allow_redirects=allow_redirects, timeout=timeout, verify=False)
            r.raise_for_status()
            r.encoding = "utf-8"
            return r
        except Exception as e2:
            log(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªÙ„Ø§Ø´ Ø¯ÙˆÙ… GET Ø¨Ø¯ÙˆÙ† SSL ({url}): {e2}")
            return None
    except requests.exceptions.RequestException as e:
        log(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± GET {url}: {e}")
        return None


# ---------- ØªØ§Ø¨Ø¹ÛŒ Ú©Ù‡ URL Ù†Ù‡Ø§ÛŒÛŒ ØµÙØ­Ù‡ Ú©ØªØ§Ø¨ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯ Ùˆ HTML Ø¢Ù† Ø±Ø§ ----------
def get_final_book_url_and_html(isbn):
    isbn_clean = str(isbn).replace("-", "").strip()
    if not isbn_clean:
        return None, None
    search_url = f"{BASE}/result/{isbn_clean}?t=Ú©ØªØ§Ø¨&s=0"
    r = safe_get(search_url)
    if not r:
        return None, None
    final_url = r.url
    html = r.text

    # Ø§Ú¯Ø± Ø±ÛŒØ¯Ø§ÛŒØ±Ú©Øª Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ù‡ /book/ Ø´Ø¯Ù‡ØŒ Ø®ÙˆØ¨ Ø§Ø³Øª
    if "/book/" in final_url:
        return final_url, html

    # ÙˆÚ¯Ø±Ù†Ù‡ Ø§Ø² ØµÙØ­Ù‡ Ù†ØªØ§ÛŒØ¬ Ø§ÙˆÙ„ÛŒÙ† Ù„ÛŒÙ†Ú© /book/ Ø±Ø§ Ø¨Ø±Ø¯Ø§Ø±
    soup = BeautifulSoup(html, "html.parser")
    link_tag = soup.find("a", href=re.compile(r"^/book/"))
    if link_tag:
        href = link_tag.get("href")
        book_page_url = (BASE + href) if href.startswith("/") else href
        r2 = safe_get(book_page_url)
        if r2:
            return book_page_url, r2.text
    return None, None

# ---------- ØªØ§Ø¨Ø¹ÛŒ Ú©Ù‡ Ø§Ø² HTML ØµÙØ­Ù‡ Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ù‡Ù…Ø§Ù† div Ù†Ø³Ø®Ù‡ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ ----------
def get_book_div_from_page(page_url, html, isbn):
    """
    ÙˆØ±ÙˆØ¯ÛŒ: page_url (Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø´Ø§Ù…Ù„ fragment Ù…Ø«Ù„ #p-114766)ØŒ html ØµÙØ­Ù‡ØŒ Ùˆ Ø´Ø§Ø¨Ú© Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø±
    Ø®Ø±ÙˆØ¬ÛŒ: bs4 Tag Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ù‡Ù…Ø§Ù† Ù†Ø³Ø®Ù‡ (div) ÛŒØ§ None Ø§Ú¯Ø± ØµÙØ­Ù‡ ØªÚ©â€ŒÚ©ØªØ§Ø¨ ÛŒØ§ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.
    Ø±ÙØªØ§Ø±:
      - Ø§Ú¯Ø± fragment (#p-XXXX) ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´Øª Ø³Ø¹ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ div Ø¨Ø§ Ù‡Ù…Ø§Ù† id Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯.
      - Ø¯Ø± ØºÛŒØ± Ø§ÛŒÙ† ØµÙˆØ±ØªØŒ Ù„ÛŒØ³Øª divÙ‡Ø§ÛŒ id^="p-" Ø±Ø§ Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯ Ùˆ Ø¯Ø± Ù‡Ø± Ú©Ø¯Ø§Ù… Ø¯Ù†Ø¨Ø§Ù„ span Ø¨Ø§ 'Ø´Ø§Ø¨Ú©:' Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø¯ Ùˆ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
      - Ø§Ú¯Ø± Ù‡ÛŒÚ†ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ØŒ None Ø¨Ø§Ø²Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯ (Ø­Ø§Ù„Øª ØªÚ©â€ŒÚ©ØªØ§Ø¨ ÛŒØ§ Ù†Ø§Ù…Ø´Ø®Øµ).
    """
    if not html:
        return None
    soup = BeautifulSoup(html, "html.parser")

    # Ø¨Ø±Ø±Ø³ÛŒ fragment Ø¯Ø± URL (Ù…Ø«Ù„Ø§Ù‹ #p-30809)
    frag = urlparse(page_url).fragment if page_url else ""
    if frag:
        # ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† div Ø¨Ø§ Ù‡Ù…Ø§Ù† id
        candidate = soup.find(id=frag)
        if candidate:
            log(f" -> ÛŒØ§ÙØªÙ† div Ø¨Ø§ fragment: #{frag}")
            return candidate

    # Ù„ÛŒØ³Øª Ú©Ø§Ù†Ø¯ÛŒØ¯Ù‡Ø§: div Ù‡Ø§ÛŒÛŒ Ú©Ù‡ id Ø´Ø§Ù† Ø¨Ø§ p- Ø´Ø±ÙˆØ¹ Ù…ÛŒâ€ŒØ´ÙˆØ¯ ÛŒØ§ Ú©Ù„Ø§Ø³ wrapper-product-acts Ø¯Ø§Ø®Ù„Ø´Ø§Ù† data-id Ø¯Ø§Ø±Ø¯
    candidates = soup.select('div[id^="p-"], div.flex.gap-2.mb-3, div.wrapper-product-acts')
    # ØªØ­ÙˆÛŒÙ„ ÙÛŒÙ„ØªØ± Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ú©Ø§Ø±Øªâ€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ: Ø§ØºÙ„Ø¨ structure Ø¯Ø§Ø±Ø§ÛŒ id="p-XXXX" Ø§Ø³Øª Ùˆ Ø¯Ø± Ú©Ù†Ø§Ø± Ø¢Ù† Ø¨Ø®Ø´ Ù…ØªØ§ Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³Øª
    # Ø§Ù…Ø§ safest approach: Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ØªÙ…Ø§Ù… divÙ‡Ø§ÛŒÛŒ Ú©Ù‡ id Ø´Ø±ÙˆØ¹ Ø¨Ø§ p- Ø¯Ø§Ø±Ù†Ø¯
    candidates = soup.select('div[id^="p-"]')
    if not candidates:
        # ØµÙØ­Ù‡ Ù…Ù…Ú©Ù† Ø§Ø³Øª ØªÚ©â€ŒÚ©ØªØ§Ø¨ Ø¨Ø§Ø´Ø¯
        log(" -> ØµÙØ­Ù‡ Ø­Ø§ÙˆÛŒ div[id^='p-'] Ù†ÛŒØ³Øª â€” Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ ØµÙØ­Ù‡ ØªÚ©â€ŒÚ©ØªØ§Ø¨.")
        return None

    clean_isbn = str(isbn).replace("-", "").strip()
    log(f" -> {len(candidates)} div Ø¨Ø§ id^='p-' Ù¾ÛŒØ¯Ø§ Ø´Ø¯Ø› Ø¯Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØ·Ø§Ø¨Ù‚ Ø´Ø§Ø¨Ú© {clean_isbn} ...")
    for div in candidates:
        # Ø¯Ø± Ø¯Ø§Ø®Ù„ Ù‡Ø± div Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ ÛŒÚ© span Ø¨Ø§ Ù…ØªÙ† "Ø´Ø§Ø¨Ú©:" ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ Ùˆ sibling Ø¢Ù† Ù…Ù‚Ø¯Ø§Ø± Ø´Ø§Ø¨Ú© Ø§Ø³Øª
        isbn_span = div.find(lambda t: t.name in ["span","div"] and isinstance(t.string, str) and "Ø´Ø§Ø¨Ú©" in t.string)
        if isbn_span:
            # Ø³Ø¹ÛŒ Ú©Ù†ÛŒÙ… Ù…Ù‚Ø¯Ø§Ø± Ø¨Ø¹Ø¯ÛŒ ÛŒØ§ sibling Ø±Ø§ Ø¨Ø®ÙˆØ§Ù†ÛŒÙ…
            # Ø­Ø§Ù„Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù: <span>Ø´Ø§Ø¨Ú©:</span><span>978-...</span>
            val = None
            nxt = isbn_span.find_next_sibling()
            if nxt and isinstance(nxt.get_text(strip=True), str) and nxt.get_text(strip=True):
                val = nxt.get_text(strip=True)
            else:
                # ÛŒØ§ Ø³Ø§Ø®ØªØ§Ø± Ø¯ÛŒÚ¯Ø±: isbn_span.parent.find(...) 
                possible = isbn_span.parent.find_all("span")
                for s in possible:
                    txt = s.get_text(strip=True)
                    if re.search(r"\d{9,13}", txt):
                        val = txt
                        break
            if val:
                if str(val).replace("-", "").strip() == clean_isbn:
                    log(f" -> ØªØ·Ø§Ø¨Ù‚ Ø´Ø§Ø¨Ú© Ø¯Ø± div id='{div.get('id')}' ÛŒØ§ÙØª Ø´Ø¯ ({val}).")
                    return div
    log(" -> Ù‡ÛŒÚ† div Ù…Ù†Ø·Ø¨Ù‚ Ø¨Ø§ Ø´Ø§Ø¨Ú© Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯Ø› Ø¨Ø§Ø²Ú¯Ø´Øª None (Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ ØµÙØ­Ù‡ ØªÚ©â€ŒÚ©ØªØ§Ø¨ ÛŒØ§ Ø³Ø§Ø®ØªØ§Ø± Ù…ØªÙØ§ÙˆØª).")
    return None

    # --- ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ù…Ø§Ø±Ù‡ Ø¬Ù„Ø¯ ---
    def infer_collection_number(main_title: str, sub_title: str):
        # ØªØ±Ú©ÛŒØ¨ Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¬Ø§Ù…Ø¹ØŒ Ø§Ù…Ø§ Ø§ÙˆÙ„ÙˆÛŒØª Ø¬Ø³ØªØ¬Ùˆ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø±ÙˆÛŒ sub_title Ø§Ø³Øª
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø§Ø¹Ø¯Ø§Ø¯ ÙØ§Ø±Ø³ÛŒ Ø¨Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ
        def normalize_digits(txt):
            if not txt: return ""
            replacements = {'Û°': '0', 'Û±': '1', 'Û²': '2', 'Û³': '3', 'Û´': '4', 'Ûµ': '5', 'Û¶': '6', 'Û·': '7', 'Û¸': '8', 'Û¹': '9'}
            for k, v in replacements.items():
                txt = txt.replace(k, v)
            return txt

        clean_sub = normalize_digits(sub_title)
        clean_main = normalize_digits(main_title)
        
        # 1. Ø§ÙˆÙ„ÙˆÛŒØª Ø§ÙˆÙ„: Ø¨Ø§Ø²Ù‡ Ø¹Ø¯Ø¯ÛŒ Ø¯Ø§Ø®Ù„ Ù¾Ø±Ø§Ù†ØªØ² Ù…Ø«Ù„ (11-10) ÛŒØ§ (10-11)
        # Ù…Ø«Ø§Ù„: Ú©ØªØ§Ø¨ Ø§ÛŒÙ„ÛŒØ§ (11-10)
        range_match = re.search(r"\((\d+)\s*-\s*(\d+)\)", clean_sub)
        if range_match:
            n1, n2 = range_match.group(1), range_match.group(2)
            # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø§Ø¹Ø¯Ø§Ø¯ (Ú©Ù‡ Ù‡Ù…ÛŒØ´Ù‡ 10 Ùˆ 11 Ø¨Ø§Ø´Ø¯ Ù†Ù‡ 11 Ùˆ 10)
            nums = sorted([int(n1), int(n2)])
            return f"{nums[0]} Ùˆ {nums[1]}"

        # 2. Ø§ÙˆÙ„ÙˆÛŒØª Ø¯ÙˆÙ…: Ø¹Ø¯Ø¯ Ø¯Ø§Ø®Ù„ Ù¾Ø±Ø§Ù†ØªØ² (Ø¨Ø§ ÛŒØ§ Ø¨Ø¯ÙˆÙ† Ù…ØªÙ†)
        # Ù…Ø«Ø§Ù„: (12) ÛŒØ§ (Ù†Ú¯Ù‡Ø¨Ø§Ù†Ø§Ù† Ú¯Ø§Ù‡ÙˆÙ„ 3) ÛŒØ§ (5)
        # Ù¾ØªØ±Ù†: Ù¾Ø±Ø§Ù†ØªØ² Ø¨Ø§Ø² -> (Ø§Ø®ØªÛŒØ§Ø±ÛŒ: Ù‡Ø± Ù…ØªÙ†ÛŒ) -> Ø¹Ø¯Ø¯ -> Ù¾Ø±Ø§Ù†ØªØ² Ø¨Ø³ØªÙ‡
        paren_match = re.search(r"\((?:[^)]*?\s+)?(\d{1,3})\)", clean_sub)
        if paren_match:
            return paren_match.group(1)

        # 3. Ø§ÙˆÙ„ÙˆÛŒØª Ø³ÙˆÙ…: Ø¬Ø¯Ø§Ú©Ù†Ù†Ø¯Ù‡ Ø®Ø§Øµ Ù…Ø«Ù„ "_" ÛŒØ§ "-" Ù‚Ø¨Ù„ Ø§Ø² Ø¹Ø¯Ø¯
        # Ù…Ø«Ø§Ù„: Ù‡ÙØª Ù†Ø´Ø§Ù†Ù‡ _ 4
        sep_match = re.search(r"[_\-]\s*(\d{1,3})\b", clean_sub)
        if sep_match:
            return sep_match.group(1)

        # 4. Ø§ÙˆÙ„ÙˆÛŒØª Ú†Ù‡Ø§Ø±Ù…: Ø¹Ø¯Ø¯ Ø¯Ø± Ø§Ù†ØªÙ‡Ø§ÛŒ Ù…ØªÙ† (Ø¨Ø§ ÙØ§ØµÙ„Ù‡ Ø§Ø² Ú©Ù„Ù…Ù‡ Ù‚Ø¨Ù„)
        # Ù…Ø«Ø§Ù„: Ù‚ØµÙ‡ Ù‡Ø§ÛŒ ÙÙ„ÛŒÚ©Ø³ 5
        # Ø´Ø±Ø·: Ø¹Ø¯Ø¯ Ø¨Ø§ÛŒØ¯ 1 ØªØ§ 3 Ø±Ù‚Ù… Ø¨Ø§Ø´Ø¯ (ØªØ§ Ø³Ø§Ù„ Ø§Ù†ØªØ´Ø§Ø± Ù…Ø«Ù„ 1399 Ø§Ø´ØªØ¨Ø§Ù‡ Ù†Ø´ÙˆØ¯)
        trailing_match = re.search(r"\s+(\d{1,3})$", clean_sub.strip())
        if trailing_match:
            return trailing_match.group(1)

        # 5. Ø§ÙˆÙ„ÙˆÛŒØª Ù¾Ù†Ø¬Ù…: ØªØ·Ø¨ÛŒÙ‚ Ø¨Ø§ Ø¹Ù†ÙˆØ§Ù† Ø§ØµÙ„ÛŒ (Ø±ÙˆØ´ Ù‚Ø¯ÛŒÙ…ÛŒ)
        # Ù…Ø«Ø§Ù„: Ø§Ú¯Ø± Ø¹Ù†ÙˆØ§Ù† Ø§ØµÙ„ÛŒ "ØªÙ‡ Ø¬Ø¯ÙˆÙ„ÛŒ Ù‡Ø§" Ø¨Ø§Ø´Ø¯ Ùˆ Ø¹Ù†ÙˆØ§Ù† ÙØ±Ø¹ÛŒ "ØªÙ‡ Ø¬Ø¯ÙˆÙ„ÛŒ Ù‡Ø§ 2"
        if clean_main:
            stripped_main = re.escape(clean_main.strip())
            main_pattern = rf"{stripped_main}\s*(\d+)"
            m_main = re.search(main_pattern, clean_sub)
            if m_main:
                return m_main.group(1)

        return ""


# ---------- ØªØ§Ø¨Ø¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ø²Ø¦ÛŒØ§Øª ÙÙ‚Ø· Ø§Ø² Ù‡Ù…Ø§Ù† div (ÛŒØ§ Ø¯Ø± ØµÙˆØ±Øª None Ø§Ø² ØµÙØ­Ù‡ Ú©Ù„ÛŒ) ----------
def extract_details_from_div(div, soup, isbn):
    info = {}

    # --- ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ: ÙÙ‚Ø· Ø´Ù…Ø§Ø±Ù‡ Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ (Ù…ØªÙ† Ø±Ø§ ØªØºÛŒÛŒØ± Ù†Ù…ÛŒâ€ŒØ¯Ù‡Ø¯) ---
    def detect_collection_number(text):
        if not text: return ""
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ÙˆÙ‚Øª Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ
        temp_text = text.strip()
        replacements = {'Û°': '0', 'Û±': '1', 'Û²': '2', 'Û³': '3', 'Û´': '4', 'Ûµ': '5', 'Û¶': '6', 'Û·': '7', 'Û¸': '8', 'Û¹': '9'}
        for k, v in replacements.items():
            temp_text = temp_text.replace(k, v)

        # 1. Ø§ÙˆÙ„ÙˆÛŒØª Ø§ÙˆÙ„: Ø§Ù„Ú¯ÙˆÛŒ "Ø¹Ø¯Ø¯ Ùˆ Ø¹Ø¯Ø¯" (Ù…Ø«Ù„: 6 Ùˆ 7)
        m_and = re.search(r"(\d+)\s*Ùˆ\s*(\d+)", temp_text)
        if m_and:
            nums = sorted([int(m_and.group(1)), int(m_and.group(2))])
            return f"{nums[0]} Ùˆ {nums[1]}"

        # 2. Ø§Ù„Ú¯ÙˆÛŒ Ø¨Ø§Ø²Ù‡: (11-10)
        m_range = re.search(r"\((\d+)\s*-\s*(\d+)\)", temp_text)
        if m_range:
            nums = sorted([int(m_range.group(1)), int(m_range.group(2))])
            return f"{nums[0]} Ùˆ {nums[1]}"

        # 3. Ø§Ù„Ú¯ÙˆÛŒ Ù¾Ø±Ø§Ù†ØªØ² ØªÚ©ÛŒ: (12)
        m_paren = re.search(r"\((?:[^)]*?\s+)?(\d{1,3})\)", temp_text)
        if m_paren: return m_paren.group(1)

        # 4. Ø§Ù„Ú¯ÙˆÛŒ Ø¬Ø¯Ø§Ú©Ù†Ù†Ø¯Ù‡ Ø§Ù†ØªÙ‡Ø§ÛŒ Ø®Ø·: _ 4 ÛŒØ§ - 4
        m_sep = re.search(r"[_\-]\s*(\d{1,3})$", temp_text)
        if m_sep: return m_sep.group(1)
        
        # 5. Ø§Ù„Ú¯ÙˆÛŒ Ø¯Ùˆ Ù†Ù‚Ø·Ù‡: " : 1"
        m_colon = re.search(r":\s*(\d{1,3})", temp_text)
        if m_colon: return m_colon.group(1)

        # 6. Ø¹Ø¯Ø¯ Ø®Ø§Ù„ÛŒ Ø¯Ø± Ø§Ù†ØªÙ‡Ø§ÛŒ Ù…ØªÙ† (Ø±ÛŒØ³Ú© Ø¯Ø§Ø±Ø¯ ÙˆÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒÙ‡Ø§ Ù…Ø¹Ù…ÙˆÙ„ Ø§Ø³Øª)
        m_trail = re.search(r"\s+(\d{1,3})$", temp_text)
        if m_trail: return m_trail.group(1)

        return ""

    # --- ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø®ÙˆØ§Ù†Ø¯Ù† Ø¬ÙØª Ú©Ù„ÛŒØ¯/Ù…Ù‚Ø¯Ø§Ø± ---
    def read_kv_pairs(block):
        kv = {}
        for row in block.select("div.flex.gap-1, div.flex.gap-2, div.flex.gap-3, div.flex.gap-4, div.flex.gap-0"):
            spans = row.find_all("span")
            if len(spans) >= 2:
                key = spans[0].get_text(strip=True).replace(":", "")
                val = spans[1].get_text(strip=True)
                kv[key] = val
        return kv

    if div is not None:
        try:
            # 1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†Ø§ÙˆÛŒÙ† Ø®Ø§Ù…
            h2_tag = div.find(lambda t: t.name in ["h2","h3"] and t.get_text(strip=True))
            if not h2_tag: h2_tag = div.find(itemprop="name")
            raw_h2 = h2_tag.get_text(strip=True) if h2_tag else ""

            sub_tag = h2_tag.find_next_sibling('div') if h2_tag else None
            raw_sub = ""
            if sub_tag and 'ltr' not in sub_tag.get('class', []):
                raw_sub = sub_tag.get_text(strip=True)

            # 2. Ø«Ø¨Øª Ø¯Ø± Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ (Ø·Ø¨Ù‚ Ø¯Ø³ØªÙˆØ±: ÙÙ‚Ø· Ø­Ø°Ù Ú©Ù„Ù…Ù‡ "Ú©ØªØ§Ø¨" Ø§Ø² Ø¹Ù†ÙˆØ§Ù† Ø§ØµÙ„ÛŒ)
            # Ø¹Ù†ÙˆØ§Ù† Ø§ØµÙ„ÛŒ
            final_main_title = re.sub(r"^Ú©ØªØ§Ø¨\s+", "", raw_h2).strip()
            info["Ø¹Ù†ÙˆØ§Ù† Ø§ØµÙ„ÛŒ"] = final_main_title
            
            # Ø¹Ù†ÙˆØ§Ù† ÙØ±Ø¹ÛŒ (Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ±)
            info["Ø¹Ù†ÙˆØ§Ù† ÙØ±Ø¹ÛŒ"] = raw_sub

            # 3. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ù…Ø§Ø±Ù‡ (Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø²ÛŒØ±Ù†ÙˆÛŒØ³ØŒ Ø§Ú¯Ø± Ù†Ø¨ÙˆØ¯ Ø¯Ø± Ø¹Ù†ÙˆØ§Ù† Ø§ØµÙ„ÛŒ)
            found_num = detect_collection_number(raw_sub)
            if not found_num:
                found_num = detect_collection_number(raw_h2)
            
            info["Ø´Ù…Ø§Ø±Ù‡ Ø¯Ø± Ù…Ø¬Ù…ÙˆØ¹Ù‡"] = found_num

        except Exception as e:
            print(f"Error extracting title: {e}")
            info["Ø¹Ù†ÙˆØ§Ù† Ø§ØµÙ„ÛŒ"] = ""
            info["Ø¹Ù†ÙˆØ§Ù† ÙØ±Ø¹ÛŒ"] = ""
            info["Ø´Ù…Ø§Ø±Ù‡ Ø¯Ø± Ù…Ø¬Ù…ÙˆØ¹Ù‡"] = ""

        # 4. ØªØµÙˆÛŒØ±
        a_img = div.find("a", href=re.compile(r"/Images/ProductImages/|/Files/AttachFiles/"))
        img_url = ""
        if a_img and a_img.get("href"):
            img_url = a_img.get("href")
        else:
            img_tag = div.find("img", itemprop="image")
            if img_tag and img_tag.get("src"):
                img_url = img_tag.get("src")

        if img_url:
            full_url = img_url if img_url.startswith("http") else (BASE + img_url)
            info["image_url"] = full_url
            info["iranketabImageName"] = os.path.basename(urlparse(full_url).path)
        else:
            info["image_url"] = ""
            info["iranketabImageName"] = ""

        # 5. Ù‚ÛŒÙ…Øª
        final_price = ""
        old_price_tag = div.find("s", class_=lambda c: c and "price" in c)
        if old_price_tag:
            final_price = re.sub(r'\D', '', old_price_tag.get_text(strip=True))
        if not final_price:
            current_price_tag = div.find(class_="toman")
            if current_price_tag:
                final_price = re.sub(r'\D', '', current_price_tag.get_text(strip=True))
            else:
                alt = div.select_one(".price, .product-price")
                if alt: final_price = re.sub(r'\D', '', alt.get_text(strip=True))
        info["Ù‚ÛŒÙ…Øª"] = final_price

        # 6. ØªØ§Ø±ÛŒØ®â€ŒÙ‡Ø§ Ùˆ Ø³Ø§ÛŒØ± ÙÛŒÙ„Ø¯Ù‡Ø§
        def get_val(cont, lbl):
            t = cont.find(lambda x: x.name=="span" and lbl in x.get_text(strip=True))
            return t.find_next_sibling("span").get_text(strip=True) if t and t.find_next_sibling("span") else ""

        info["Ø³Ø§Ù„ Ø§Ù†ØªØ´Ø§Ø± Ø´Ù…Ø³ÛŒ"] = get_val(div, "Ø³Ø§Ù„ Ø§Ù†ØªØ´Ø§Ø± Ø´Ù…Ø³ÛŒ")
        info["Ø³Ø§Ù„ Ø§Ù†ØªØ´Ø§Ø± Ù…ÛŒÙ„Ø§Ø¯ÛŒ"] = get_val(div, "Ø³Ø§Ù„ Ø§Ù†ØªØ´Ø§Ø± Ù…ÛŒÙ„Ø§Ø¯ÛŒ")

        def find_lbl(d, l):
            s = d.find(lambda t: t.name in ["span","div"] and isinstance(t.string, str) and l in t.get_text())
            if s:
                a = s.find_next("a")
                if a: return a.get_text(strip=True)
                sib = s.find_next_sibling()
                if sib: return sib.get_text(strip=True)
            return ""

        info["Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡"] = find_lbl(div, "Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡:")
        info["Ù…ØªØ±Ø¬Ù…"] = find_lbl(div, "Ù…ØªØ±Ø¬Ù…:")
        info["Ù†Ø§Ø´Ø±"] = find_lbl(div, "Ø§Ù†ØªØ´Ø§Ø±Ø§Øª:")

        orig = soup.find("div", class_=lambda c: c and "ltr" in c)
        info["Ø¹Ù†ÙˆØ§Ù† Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ"] = orig.get_text(strip=True) if orig else ""

        kv = {}
        for block in div.select("div.card, div.absolute, div.flex"): kv.update(read_kv_pairs(block))
        if not kv: kv.update(read_kv_pairs(div))
        for k, v in kv.items(): info[k] = v

        if "Ø´Ø§Ø¨Ú©" not in info:
            isbn_span = div.find(lambda t: t.name in ["span","div"] and isinstance(t.string, str) and "Ø´Ø§Ø¨Ú©" in t.get_text())
            if isbn_span:
                nxt = isbn_span.find_next_sibling()
                if nxt: info["Ø´Ø§Ø¨Ú©"] = nxt.get_text(strip=True)

        # -------------------------------------------------------
        pages = ""
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ØªÚ¯ÛŒ Ú©Ù‡ Ù…ØªÙ† "ØªØ¹Ø¯Ø§Ø¯ ØµÙØ­Ù‡" Ø¯Ø§Ø±Ø¯
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² re.compile Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…ØªÙ† Ø­ØªÛŒ Ø§Ú¯Ø± ÙØ§ØµÙ„Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯
        page_label = soup.find("span", string=re.compile("ØªØ¹Ø¯Ø§Ø¯ ØµÙØ­Ù‡"))
        if page_label:
            # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ØªÚ¯ Ø¨Ø¹Ø¯ÛŒ (Ù‡Ù…Ø³Ø§ÛŒÙ‡) Ú©Ù‡ Ø­Ø§ÙˆÛŒ Ø¹Ø¯Ø¯ Ø§Ø³Øª
            page_val_span = page_label.find_next_sibling("span")
            if page_val_span:
                pages = page_val_span.get_text(strip=True)
        
        info["ØµÙØ­Ø§Øª"] = pages
        # -------------------------------------------------------
        
        return info
    return {}



# ---------- pywin32 helper ----------
def ensure_pywin32():
    try:
        import win32com.client as win32
        return win32
    except Exception:
        raise SystemExit("âŒ Ù†ÛŒØ§Ø² Ø¨Ù‡ pywin32 (python -m pip install pywin32)")

# ---------- Ø¯Ø§Ù†Ù„ÙˆØ¯ ØªØµÙˆÛŒØ± (Ø¨Ø§ Ø§ØµÙ„Ø§Ø­ SSL) ----------
def download_image(img_url, isbn):
    if not img_url:
        return None
    isbn_clean = str(isbn).replace("-", "").strip()
    filename = f"{isbn_clean}.jpg"
    path = os.path.join(IMAGE_DIR, filename)
    if os.path.exists(path):
        return path
    
    try:
        # ØªÙ„Ø§Ø´ Ø§ÙˆÙ„: Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯
        r = requests.get(img_url, headers=HEADERS, stream=True, timeout=30, verify=True)
    except requests.exceptions.SSLError:
        # ØªÙ„Ø§Ø´ Ø¯ÙˆÙ…: Ø§Ú¯Ø± Ø®Ø·Ø§ÛŒ SSL Ø¯Ø§Ø¯ØŒ Ø¨Ø¯ÙˆÙ† Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù…Ù†ÛŒØªÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù†
        try:
            r = requests.get(img_url, headers=HEADERS, stream=True, timeout=30, verify=False)
        except Exception:
            return None
    except Exception:
        return None

    try:
        r.raise_for_status()
        with open(path, "wb") as f:
            for chunk in r.iter_content(8192):
                f.write(chunk)
        return path
    except Exception:
        return None

# ---------------------------------------------------------
# ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ (MAIN)
# ---------------------------------------------------------
def main():
    win32 = ensure_pywin32()
    try:
        # ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ§Ù†Ø¯Ù† ØªØ¹Ø¯Ø§Ø¯ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ Ùˆ Ø§ÛŒÙ†Ø¯Ú©Ø³â€ŒÙ‡Ø§ Ø§Ø² Ù¾Ø§Ù†Ø¯Ø§Ø³ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
        df_source = pd.read_excel(EXCEL_FILE, sheet_name=0, dtype=str).fillna("")
    except Exception as e:
        raise SystemExit(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„: {e}")

    excel = win32.gencache.EnsureDispatch("Excel.Application")
    excel.Visible = True  # Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø±ÙˆÙ†Ø¯ Ú©Ø§Ø±
    wb = excel.Workbooks.Open(os.path.abspath(EXCEL_FILE))
    ws = wb.Worksheets(1)

    # 1. Ù†Ù‚Ø´Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ Ø§Ø² Ù‡Ø¯Ø±Ù‡Ø§ (Header Map)
    used_cols = ws.UsedRange.Columns.Count
    header_map = {}
    # Ø±Ø¯ÛŒÙ 1 Ø±Ø§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†ÛŒÙ…
    for c in range(1, used_cols + 20): # Ú©Ù…ÛŒ Ø¨ÛŒØ´ØªØ± Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†ÛŒÙ… ØªØ§ Ù…Ø·Ù…Ø¦Ù† Ø´ÙˆÛŒÙ…
        val = ws.Cells(1, c).Value
        if val:
            header_map[str(val).strip()] = c
        else:
            # Ø§Ú¯Ø± Ø¨Ù‡ Ø³Ù„ÙˆÙ„ Ø®Ø§Ù„ÛŒ Ø±Ø³ÛŒØ¯ÛŒÙ… Ùˆ Ø¨Ø¹Ø¯Ø´ Ù‡Ù… Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯ØŒ ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªÙ…Ø§Ù… Ø´Ø¯Ù‡
            # (Ø§Ù…Ø§ Ú†ÙˆÙ† Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒÙ… Ø³ØªÙˆÙ† Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒÙ…ØŒ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±ÛŒÙ† Ø±Ø§Ù‡ UsedRange Ø¨ÙˆØ¯ Ú©Ù‡ Ø¨Ø§Ù„Ø§ Ø²Ø¯ÛŒÙ…)
            pass

    # 2. Ù„ÛŒØ³Øª Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø§Ø¬Ø¨Ø§Ø±ÛŒ Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ø¨Ø§Ø´Ù†Ø¯ (Ø§Ú¯Ø± Ù†Ø¨Ø§Ø´Ù†Ø¯ Ù…ÛŒâ€ŒØ³Ø§Ø²ÛŒÙ…)
    required_cols = [
        "ØªØµÙˆÛŒØ±", 
        "Ù‚ÛŒÙ…Øª", 
        "iranketabImageName", 
        "Ø¹Ù†ÙˆØ§Ù† Ø§ØµÙ„ÛŒ", 
        "Ø¹Ù†ÙˆØ§Ù† ÙØ±Ø¹ÛŒ", 
        "Ø´Ù…Ø§Ø±Ù‡ Ø¯Ø± Ù…Ø¬Ù…ÙˆØ¹Ù‡",
        "Ø³Ø§Ù„ Ø§Ù†ØªØ´Ø§Ø± Ø´Ù…Ø³ÛŒ",
        "Ø³Ø§Ù„ Ø§Ù†ØªØ´Ø§Ø± Ù…ÛŒÙ„Ø§Ø¯ÛŒ",
        "ØµÙØ­Ø§Øª"  # <--- Ø§ÛŒÙ† Ù…ÙˆØ±Ø¯ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯
    ]

    # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¢Ø®Ø±ÛŒÙ† Ø³ØªÙˆÙ† Ù¾Ø± Ø´Ø¯Ù‡
    last_col_idx = 0
    if header_map:
        last_col_idx = max(header_map.values())
    
    for req in required_cols:
        if req not in header_map:
            last_col_idx += 1
            ws.Cells(1, last_col_idx).Value = req
            header_map[req] = last_col_idx
            log(f" + Ø³ØªÙˆÙ† Ø¬Ø¯ÛŒØ¯ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯: {req}")

    if "Ø´Ø§Ø¨Ú©" not in header_map:
        wb.Close(SaveChanges=False)
        excel.Quit()
        raise SystemExit("âŒ Ø³ØªÙˆÙ† 'Ø´Ø§Ø¨Ú©' Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.")

    total_rows = len(df_source)
    log(f"Ø´Ø±ÙˆØ¹ Ù¾Ø±Ø¯Ø§Ø²Ø´ {total_rows} Ø±Ú©ÙˆØ±Ø¯...")

    # 3. Ø­Ù„Ù‚Ù‡ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
    for idx, row_data in df_source.iterrows():
        excel_row = int(idx) + 2  # Ú†ÙˆÙ† Ù‡Ø¯Ø± Ø±Ø¯ÛŒÙ 1 Ø§Ø³Øª Ùˆ Ù¾Ø§Ù†Ø¯Ø§Ø³ Ø§Ø² 0 Ø´Ø±ÙˆØ¹ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
        isbn = str(row_data.get("Ø´Ø§Ø¨Ú©", "")).strip()

        if not isbn:
            continue

        # --- Ø´Ø±Ø· Ù¾Ø±Ø´ (Skip Logic) ---
        # Ø§Ú¯Ø± UPDATE_ALL Ø®Ø§Ù…ÙˆØ´ Ø§Ø³ØªØŒ Ú†Ú© Ú©Ù† Ø¢ÛŒØ§ "Ø¹Ù†ÙˆØ§Ù† Ø§ØµÙ„ÛŒ" Ù¾Ø± Ø§Ø³ØªØŸ
        if not UPDATE_ALL:
            # Ù…Ù‚Ø¯Ø§Ø± ÙØ¹Ù„ÛŒ Ø³Ù„ÙˆÙ„ Ø¹Ù†ÙˆØ§Ù† Ø§ØµÙ„ÛŒ Ø±Ø§ Ø§Ø² Ø§Ú©Ø³Ù„ Ø²Ù†Ø¯Ù‡ Ø¨Ø®ÙˆØ§Ù† (Ù†Ù‡ Ø§Ø² Ù¾Ø§Ù†Ø¯Ø§Ø³ Ú©Ù‡ Ø´Ø§ÛŒØ¯ Ù‚Ø¯ÛŒÙ…ÛŒ Ø¨Ø§Ø´Ø¯)
            current_main_title = ws.Cells(excel_row, header_map["Ø¹Ù†ÙˆØ§Ù† Ø§ØµÙ„ÛŒ"]).Value
            # Ø§Ú¯Ø± Ø³Ù„ÙˆÙ„ Ù¾Ø± Ø¨ÙˆØ¯ (None Ù†Ø¨ÙˆØ¯ Ùˆ Ø±Ø´ØªÙ‡ Ø®Ø§Ù„ÛŒ Ù†Ø¨ÙˆØ¯)
            if current_main_title and str(current_main_title).strip():
                # Ù„Ø§Ú¯ Ù†Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ÛŒØ§ Ú©ÙˆØªØ§Ù‡ Ù„Ø§Ú¯ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ Ø´Ù„ÙˆØº Ù†Ø´ÙˆØ¯
                # log(f"Ø±Ø¯ÛŒÙ {excel_row} Ø±Ø¯ Ø´Ø¯ (Ø¹Ù†ÙˆØ§Ù† Ø¯Ø§Ø±Ø¯).")
                continue
        # -----------------------------

        log(f"\n[Ø±Ø¯ÛŒÙ {excel_row}] ğŸ” Ø´Ø§Ø¨Ú©: {isbn}")

        # Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒÙ†Ú© Ùˆ HTML
        book_url, html = get_final_book_url_and_html(isbn)
        if not book_url or not html:
            log(" -> âŒ ØµÙØ­Ù‡ Ú©ØªØ§Ø¨ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.")
            continue

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª
        book_div = get_book_div_from_page(book_url, html, isbn)
        soup = BeautifulSoup(html, "html.parser")
        details = extract_details_from_div(book_div, soup, isbn)

        if not details:
            log(" -> âŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø´Ø¯.")
            continue

        # 4. Ø¯Ø±Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ØªÙ†ÛŒ Ø¯Ø± Ø§Ú©Ø³Ù„
        for key, val in details.items():
            if key == "ØªØµÙˆÛŒØ±": continue # ØªØµÙˆÛŒØ± Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ù‡Ù†Ø¯Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯
            
            if key in header_map:
                col_idx = header_map[key]
                # Ù†ÙˆØ´ØªÙ† Ù…Ù‚Ø¯Ø§Ø± Ø¯Ø± Ø³Ù„ÙˆÙ„
                ws.Cells(excel_row, col_idx).Value = val

        # 5. Ù…Ø¯ÛŒØ±ÛŒØª ØªØµÙˆÛŒØ± (Ø¯Ø§Ù†Ù„ÙˆØ¯ØŒ Ø­Ø°Ù Ù‚Ø¨Ù„ÛŒØŒ Ø¯Ø±Ø¬ Ø¬Ø¯ÛŒØ¯)
        img_url = details.get("image_url", "")
        if img_url:
            local_img_path = download_image(img_url, isbn)
            if local_img_path:
                img_col = header_map["ØªØµÙˆÛŒØ±"]
                
                # Ø§Ù„Ù) Ø­Ø°Ù ØªØµØ§ÙˆÛŒØ± Ù‚Ø¯ÛŒÙ…ÛŒ Ø§ÛŒÙ† Ø³Ù„ÙˆÙ„
                remove_old_images(ws, excel_row, img_col)

                # Ø¨) Ø¯Ø±Ø¬ ØªØµÙˆÛŒØ± Ø¬Ø¯ÛŒØ¯
                try:
                    cell_target = ws.Cells(excel_row, img_col)
                    left = cell_target.Left
                    top = cell_target.Top
                    width = cell_target.Width
                    height = cell_target.Height
                    
                    # AddPicture(Filename, LinkToFile, SaveWithDocument, Left, Top, Width, Height)
                    pic = ws.Shapes.AddPicture(
                        os.path.abspath(local_img_path), 
                        False, 
                        True, 
                        left, top, width, height
                    )
                    pic.Placement = 1 # Move and Size
                except Exception as e:
                    log(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±Ø¬ ØªØµÙˆÛŒØ±: {e}")
            else:
                log(" -> Ø¯Ø§Ù†Ù„ÙˆØ¯ ØªØµÙˆÛŒØ± Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯.")

        # Ø°Ø®ÛŒØ±Ù‡ Ù…ÙˆÙ‚Øª Ù‡Ø± 10 Ø±Ú©ÙˆØ±Ø¯ (Ø§Ø®ØªÛŒØ§Ø±ÛŒØŒ Ø¨Ø±Ø§ÛŒ Ø§Ù…Ù†ÛŒØª Ø¨ÛŒØ´ØªØ±)
        if idx % 10 == 0:
            wb.Save()

        time.sleep(random.uniform(*DELAY_RANGE))

    # Ù¾Ø§ÛŒØ§Ù† Ú©Ø§Ø±
    wb.Save()
    log("âœ… Ù¾Ø§ÛŒØ§Ù† Ø¹Ù…Ù„ÛŒØ§Øª. ÙØ§ÛŒÙ„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")
    # wb.Close(SaveChanges=True) # Ø§Ú¯Ø± Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒØ¯ Ø¨Ø§Ø² Ø¨Ù…Ø§Ù†Ø¯ Ø§ÛŒÙ† Ø±Ø§ Ú©Ø§Ù…Ù†Øª Ú©Ù†ÛŒØ¯
    # excel.Quit()

if __name__ == "__main__":
    main()